# Load testing - an introduction

To get reliable measurements when performing load testing, the following must be taken into consideration:

* The test environment should be dedicated to load tests only.
* Routers and firewalls must not constitue bottlenecks or interpret load tests as Denial of Service attacks.

Because of the above, the load client should be physically positioned as close as possible to the Qlik SenseÂ® Enterprise deployment under test.

To benchmark the capacity of a Qlik Sense Enterprise deployment in terms of number of clicks per second or response times, load test scenarios should resemble real usage as closely as possible. So, when creating scenarios, it is important to select realistic inter-departure times (that is, think times between clicks) for the requests from virtual users. The think times in between actions are often too short and do not reflect the behavior of real users who might, for example, perform their analysis while in a meeting or a phone call or for some other reason need more think time between the actions than initially expected (this is especially important to consider when simulating large amounts of users as the actions and think times greatly impact the load and thereby the number of users that the system can accommodate without becoming saturated).

Another challenging task is to create an average scenario that replicates the load generated by many users in total when scaled up. The load can be tuned by changing:

* The number of actions in a load test scenario
* The inter-departure times between adjacent actions
* The number of concurrent virtual users

The first two bullets are covered by the design of the load test scenario, whereas the number of concurrent virtual users can be tuned during the load testing session.

## Using Gopherciser

The performance testing is based on load scenarios, which are sequences of actions carried out by virtual Qlik Sense Enterprise users. 

A load scenario is defined in a JSON file and can be executed sequentially or in parallel with other load scenarios to simulate a realistic user scenario that can be used to investigate the performance of a Qlik Sense Enterprise deployment.

### Overview of commands

`gopherciser [command]`

Commands:

* `execute` (or `x`): Run a load scenario towards a Qlik Sense Enterprise deployment.
* `help`: Show the help.
* `objdef` (or `od`): Export and validate object definitions files.
* `script` (or `s`): Execute script command.
* `version` (or `ver`): Show the version information.

Flags:

* `-h`, `--help`: Show the help for a command (`gopherciser [command] --help`).

#### Execute command

`gopherciser execute [flags]`

`gopherciser x [flags]`

Flags:

* `-c`, `--config string`: Load the specified scenario setup file.
* `--debug`: Log debug information.
* `-h`, `--help`: Show the help for the `execute` command.
* `--logformat string`: Set the specified log format. The log format specified in the scenario setup file is used by default. If no log format is specified, `tsvfile` is used.
  * `0` or `tsvfile`: TSV file
  * `1` or `tsvconsole`: TSV console
  * `2` or `jsonfile`: JSON file
  * `3` or `jsonconsole`: JSON console
  * `4` or `console`: Console
  * `5` or `combined`: Combined (TSV file + JSON console)
  * `6` or `no`: Default logs and status output turned off.
  * `7` or `onlystatus`: Default logs turned off, but status output turned on.
* `--metrics int`: Set the port to use for export of Prometheus metrics when Gopherciser is running.
* `--metricsaddress string`: (Prometheus only) Pull, if the string is empty, otherwise push. The port is appended.
* `--metricslabel string`: (Prometheus PUSH only) A label (Prometheus job) to be used when pushing metrics to remote Prometheus
* `--metricsgroupingkey key=value`, `-g key=value`: (Prometheus PUSH only) This flag, which can be supplied multiple times, sets Prometheus grouping keys (in key=value format).
* `--profile string`: Start the specified profiler.
  * `1` or `cpu`: CPU
  * `2` or `block`: Block
  * `3` or `goroutine`: Goroutine
  * `4` or `threadcreate`: Threadcreate
  * `5` or `heap`: Heap
  * `6` or `mutex`: Mutex
  * `7` or `trace`: Trace
  * `8` or `mem`: Mem
* `--summary string`: Set the type of summary to display after the test run. Defaults to `simple` for minimal performance impact.
  * `0` or `undefined`: Simple, single-row summary
  * `1` or `none`: No summary
  * `2` or `simple`: Simple, single-row summary
  * `3` or `extended`: Extended summary that includes statistics on each unique combination of `action`, `label` and `app GUID`
  * `4` or `full`: Same as `extended`, but with statistics on each unique combination of `method` and `endpoint` added
* `-t`, `--traffic`: Log traffic information. **Note:** This should only be used for debugging purposes as traffic logging is resource-consuming.
* `-m`, `--trafficmetrics`: Log metrics information.

Exit codes:

* `0`: Execution OK
* `1` - `127`: Number of errors during the execution (`127` means 127 errors or more)
* `128`: Error during the execution (ExitCodeExecutionError)
* `129`: Error when parsing the JSON config (ExitCodeJSONParseError)
* `130`: Error when validating the JSON config (ExitCodeJSONValidateError)
* `131`: Error when resolving the log format (ExitCodeLogFormatError)
* `132`: Error when reading the object definitions (ExitCodeObjectDefError)
* `133`: Error when starting the profiling (ExitCodeProfilingError)

#### Objdef command

`gopherciser objdef [sub-commands]`

`gopherciser od [sub-commands]`

Sub-commands:

* `generate`: Generate an object definitions file from the default values.
* `validate`: Validate the object definitions in a definitions file.

`generate` command flags:

* `-d`, `--definitions`: (mandatory) Name of the definitions file to create.
* `-f`, `--force`: Overwrite an existing definitions file.
* `-h`, `--help`: Show the help for the `generate` command.
* `-o`, `--object strings`: (optional) List of objects to include in the definitions file. Defaults to all.

`validate` command flags:

* `-d`, `--definitions`: (mandatory) Name of the definitions file to validate.
* `-h`, `--help`: Show the help for the `validate` command.
* `-v`, `--verbose`: Display a summary of the validation.

For more information on how to use the `objdef` command, see [Supporting extensions and overriding defaults](./sense-object-definitions.md).

#### Script command

`gopherciser script [sub-commands] [flags]`

`gopherciser s [sub-commands] [flags]`

Sub-commands:

* `validate` (or `v`): Validate a scenario script.
* `template` (or `tmpl` or `t`): Generate a template scenario script.

`validate` command flags:

* `-c`, `--config string`: Load the specified scenario setup file.
* `-h`, `--help`: Show the help for the `validate` command.

`template` command flags:

* `-c`, `--config string`: (optional) Create the specified scenario setup file. Defaults to `template.json`.
* `-f`, `--force`: Overwrite existing scenario setup file.
* `-h`, `--help`: Show the help for the `template` command.

## Analyzing the test results

A log file is recorded during each test execution. The `logs.filename` setting in the `settings` section of the load test script specifies the name of and the path to the log file (see [Setting up load scenarios](./settingup.md)). If a file with the specified filename already exists, a number is appended to the filename (for example, if the file `xxxxx.yyy` already exists, the log file is stored as `xxxxx-001.yyy`). 

The contents of the log file differ depending on the type of logging selected. Examples of rows that typically can be found in the log file include:

* `result`: The result of an action (complete with timestamp, response time and information whether or not the action was successful).
* `info`: Information related to the test execution (for example, the total number of errors, actions and requests during the test execution).
* `error`: Information related to errors during the test execution.

The test results (that is, log files) can be analyzed using the [Scalability Results Analyzer](https://community.qlik.com/t5/Qlik-Scalability/Scalability-Results-Analyzer/gpm-p/1493648).

## Complementary manual measurements

To capture the full end user experience, manual measurements are needed. A web browser in combination with an optional measurement method can be used to get a snapshot of the full response times including rendering of visualizations etc. 

Perform the actions defined in the load test scenario and measure the time for each action to complete (using, for example, Fiddler). To measure the user-perceived response times under specific load, perform the measurements while the load test scenario is executed.

## Limitations

These are the current limitations in Gopherciser:

* Not supported:
  * Variance waterfall chart
  * P&L pivot chart object
  * Trellis container extension
  * Chart suggestions (that is, auto-charts) for box plot, distribution plot, and histogram objects (to get correct measurements for these types of objects, disable chart suggestions in each such object before starting your test)
* Pivot table: 
  * The only supported selection type is `randomfromall`
  * Values are randomly selected from all values in the table
* Map: 
  * Selections can only be made in the first layer (that is, layer 0)
* Visualization bundle:
  * Selections are not supported in the Heatmap chart

## Links

- [Building a Docker image](./buildingdockerimage.md)
- [Setting up load scenarios](./settingup.md)
- [Supporting extensions and overriding defaults](./sense-object-definitions.md)
- [Running load scenarios in Kubernetes](./kubernetes-job.md)
- [Example scenarios](./examples.md)
- [Using Gopherciser for pre-caching](./pre-caching.md)
- [Using Gopherciser for availability testing](./availability-testing.md)
- [Extending Gopherciser](./extending-gopherciser.md)
